# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.
"""


import pandas as pd

import csv


recipe = pd.read_csv("recipe.csv")
recipe.head()

import pandas as pd
df_tomorrow = pd.read_csv('veg-price-analysis/veg_pred.csv')
df_tomorrow = df_tomorrow[df_tomorrow["ç”¢å“åç¨±"] != "å…¶ä»–"]
df_tomorrow

from datetime import datetime, timedelta
import pandas as pd

# è®€å– CSV
df_price = pd.read_csv("veg-price-analysis/veg_prices_history.csv")
df_price['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df_price['äº¤æ˜“æ—¥æœŸ']).dt.date

# ç¯©é¸æœ€è¿‘ä¸€å€‹æœˆ
today = datetime.today().date()
one_month_ago = today - timedelta(days=30)
df_recent = df_price[df_price['äº¤æ˜“æ—¥æœŸ'] >= one_month_ago]
df_recent = df_recent[~df_recent["ç”¢å“åç¨±"].str.contains("å…¶ä»–")]


print(f"æœ€è¿‘ä¸€å€‹æœˆè³‡æ–™ç­†æ•¸: {df_recent.shape[0]}")
df_recent

# å…ˆæ¸…ç†ç”¢å“åç¨±ï¼ˆå»ç©ºæ ¼ï¼Œå¿…è¦æ™‚å–ç¬¬ä¸€å€‹è©ï¼‰
df_recent['ç”¢å“åç¨±'] = df_recent['ç”¢å“åç¨±'].str.strip().str.split().str[0]

# ç”¨ groupby åˆ‡åˆ†
grouped = df_recent.groupby('ç”¢å“åç¨±')

# å„²å­˜æˆè®Šæ•¸ t1, t2, ...ï¼ˆæ–¹ä¾¿å¾ŒçºŒä½¿ç”¨ï¼‰
for i, (name, group) in enumerate(grouped, start=1):
    globals()[f't{i}'] = group
    print(f"ç”¢å“åç¨±: {name} å„²å­˜ç‚ºè®Šæ•¸ t{i}ï¼Œç­†æ•¸: {len(group)}")

avg_price_dict = {}

for i in range(1, len(grouped)+1):
    df_veg = globals()[f't{i}']
    avg_price_dict[df_veg['ç”¢å“åç¨±'].iloc[0]] = df_veg['åŠ æ¬Šå¹³å‡åƒ¹(å…ƒ/å…¬æ–¤)'].mean()

print(avg_price_dict)

import pandas as pd
from datetime import datetime, timedelta

# === è®€å–èœåƒ¹è³‡æ–™ï¼Œå‡è¨­å·²ç¶“ç®—å‡º avg_price_dict å­—å…¸ ===
tomorrow_price = dict(zip(df_tomorrow['ç”¢å“åç¨±'], df_tomorrow['é æ¸¬æ˜æ—¥èœåƒ¹(å…ƒ/å…¬æ–¤)']))

# === è®€å–é£Ÿè­œè³‡æ–™ ===
df_recipe = pd.read_csv("recipe.csv")  # æ ¼å¼: èœå,ä¸»è¦é£Ÿæ(ç”¨|åˆ†éš”),è¼”åŠ©é£Ÿæ(ç”¨|åˆ†éš”)

# === äº’å‹•ç¨‹å¼ ===
print("å“ˆå›‰~æˆ‘ä¾†çµ¦ä½ å»ºè­°å§")
print('è¼¸å…¥ã€Œæ˜æ—¥èœåƒ¹ã€â†’ è¼¸å‡ºå‰äº”åä¾¿å®œè”¬èœåŠæ˜æ—¥é æ¸¬åƒ¹æ ¼')
print('è¼¸å…¥ã€Œå»ºè­°é£Ÿè­œã€â†’ ç›´æ¥æä¾›å¯ç…®çš„èœè‰²åŠé£Ÿæ')

def handle_user_message(user_input):
   user_input = input("\nè«‹è¼¸å…¥æŒ‡ä»¤ï¼š").strip()

   if user_input == "æ˜æ—¥èœåƒ¹":
     # è¨ˆç®—å‰äº”åè”¬èœ
     diffs = []
     for veg, avg in avg_price_dict.items():
        if veg in tomorrow_price:
          diffs.append((veg, avg, tomorrow_price[veg]))

     # å…ˆæ‰¾ä½æ–¼å¹³å‡çš„è”¬èœ
     under_avg = [x for x in diffs if x[2] < x[1]]
     if len(under_avg) >= 5:
       selected = sorted(under_avg, key=lambda x: x[1]-x[2], reverse=True)[:5]
     else:
     # å¦‚æœä¸è¶³äº”å€‹ï¼Œå–é›¢å¹³å‡æœ€è¿‘çš„å‰äº”å€‹
       selected = sorted(diffs, key=lambda x: abs(x[1]-x[2]))[:5]

     result = "å‰äº”åè”¬èœåŠæ˜æ—¥åƒ¹æ ¼ï¼š\n"
     for veg, avg, price in selected:
        result += f"{veg} â†’ {price:.2f} å…ƒ/å…¬æ–¤\n"

   elif user_input == "å»ºè­°é£Ÿè­œ":
      if df_recipe.empty:
         return "æŠ±æ­‰ï¼Œé£Ÿè­œè³‡æ–™ä¸­æ²’æœ‰è³‡æ–™ ğŸ˜¢"

      # å–å¾—å‰äº”å€‹è”¬èœåç¨±
      top5_veggies = [veg for veg, _, _ in selected]
      for veggie in top5_veggies:
         veggie_lower = veggie.lower().strip()
         # ç¯©é¸ä¸»è¦é£Ÿæåˆ—è¡¨ä¸­åŒ…å«è©²è”¬èœ
         recipes_for_veggie = df_recipe[
            df_recipe["ä¸»è¦é£Ÿæ"].str.split("ã€").apply(lambda x: veggie in x)
              ]
         if recipes_for_veggie.empty:
            result += f"{veggie} â†’ æ²’æœ‰å°æ‡‰é£Ÿè­œ ğŸ˜¢\n"
         else:
            result += f"{veggie} çš„é£Ÿè­œï¼š\n"
            for _, row in recipes_for_veggie.iterrows():
              result += f"- {row['èœå']} â†’ éœ€è¦é£Ÿæ: {row['ä¸»è¦é£Ÿæ']} + {row['è¼”åŠ©é£Ÿæ']}\n"
      return result
   else:
      return result

"""#åˆ†è© ä»¥é˜²æœ‰äººå¤šæ‰“"""

# æª¢è¦–è³‡æ–™
#df.head(10)

# import jieba

# def cutProcess(sting):
#     result = jieba.lcut(sting)
#     result = " ".join(result)

#     return result

# df['quote'] = df['quote'].apply(cutProcess)

# df.head(5)

"""#è¨“ç·´è³‡æ–™"""

# data = df
# training_documents = data['quote'].values.astype('U')
# labels = data['category'].values.astype('U')

# #åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œåˆ†ä¸º80%è®­ç»ƒé›†ï¼Œ20%æµ‹è¯•é›†
# X_train, X_test, y_train, y_test = train_test_split(training_documents, labels, test_size=0.1, random_state=12)


# vectorizer = CountVectorizer(token_pattern='(?u)\\b\\w+\\b') # token_pattern='(?u)\\b\\w+\\b' å–®å­—ä¹Ÿè¨ˆå…¥
# x_train = vectorizer.fit_transform(X_train)

# # train
# classifier = MultinomialNB (alpha=0.01) # change model type here
# classifier.fit(x_train, y_train)

# training_documents

# for i in x_train:
#     print("#:"i)

"""#è©•ä¼°æ¨¡å‹

"""

# x_test = vectorizer.transform(X_test)
# classifier.score(x_test,y_test)

# print(X_test)
# predict(X_test)

# def predict(raw_queries,n_top=1):
#     raw_queries = [cutProcess(s) for s in raw_queries]
# #     print(raw_queries)

#     queries = vectorizer.transform(raw_queries)
#     predict =  classifier.predict_proba(queries).tolist()
#     predict = [{k:round(v,4) for k,v in zip(classifier.classes_[:3],qa[:3])} for qa in predict]
#     predict = [ sorted(dictt.items(), key=lambda d: d[1], reverse=True) for dictt in predict]
#     return predict

# example = ['æˆ‘æœ‰å•é¡Œ','ä¿®æ”¹å…¬å¸è³‡æ–™','æˆ‘æƒ³åœ¨å°ä¸­å¸‚æ±å±±è·¯é™„è¿‘æ‰¾å°é›ä¸Šå·¥ä¸Šçš„å·¥ä½œ','è¦æ€éº¼è®Šæ›´å…¬å¸é›»è©±','æ‚¨å¥½æ‡‰å¾µè€…ç‚ºä½•çœ‹ä¸åˆ°æˆ‘å€‘éœ€è¦å‡ºå·®çš„é …ç›®']

# lists = predict(example)

# for index,qa in enumerate(lists):
#     print("question:",example[index])
#     print("anser:", qa)

#     print()

# txt = input()
# predict([txt])[0]

